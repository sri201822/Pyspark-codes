{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Recommendation_movie</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2cba2a20a08>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").config(\"spark.driver.host\",\"localhost\").appName(\"Recommendation_movie\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('movie_ratings.csv', inferSchema= True, header = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "|summary|            userId|         movieId|            rating|           timestamp|\n",
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "|  count|            100836|          100836|            100836|              100836|\n",
      "|   mean|326.12756356856676|19435.2957177992| 3.501556983616962|1.2059460873684695E9|\n",
      "| stddev| 182.6184914635004|35530.9871987003|1.0425292390606342|2.1626103599513078E8|\n",
      "|    min|                 1|               1|               0.5|           828124615|\n",
      "|    max|               610|          193609|               5.0|          1537799250|\n",
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "recom = ALS(maxIter=10, regParam=0.02, userCol='userId', itemCol='movieId', ratingCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = recom.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Param(parent='ALS_ccb79194d7b0', name='coldStartStrategy', doc='strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: nan,drop.'), Param(parent='ALS_ccb79194d7b0', name='itemCol', doc='column name for item ids. Ids must be within the integer value range.'), Param(parent='ALS_ccb79194d7b0', name='predictionCol', doc='prediction column name'), Param(parent='ALS_ccb79194d7b0', name='userCol', doc='column name for user ids. Ids must be within the integer value range.')]\n"
     ]
    }
   ],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|   603|    471|   4.0| 954482443| 1.1420857|\n",
      "|   474|    471|   3.0| 974668858| 2.0798697|\n",
      "|   462|    471|   2.5|1123890831| 1.2137065|\n",
      "|   610|    471|   4.0|1479544381| 3.0456336|\n",
      "|   469|    471|   5.0| 965425364| 2.0824976|\n",
      "|   373|    471|   5.0| 846830388| 3.2065692|\n",
      "|   357|    471|   3.5|1348627082| 3.7114053|\n",
      "|   609|    833|   3.0| 847221080|  0.880474|\n",
      "|   387|   1088|   1.5|1095040878| 2.8107529|\n",
      "|   307|   1088|   3.0|1186162146| 2.6117928|\n",
      "|    84|   1088|   3.0| 860398568| 3.5348916|\n",
      "|   226|   1088|   1.0|1096420160|  3.182537|\n",
      "|    68|   1088|   3.5|1158534614| 2.7108884|\n",
      "|   116|   1088|   4.5|1337195649| 3.1035523|\n",
      "|   483|   1088|   3.0|1215895737| 3.5715861|\n",
      "|   503|   1342|   1.0|1335214611| 3.1918712|\n",
      "|   307|   1342|   2.0|1186087552| 2.6360106|\n",
      "|   137|   1580|   3.5|1204859475| 3.3717048|\n",
      "|   593|   1580|   1.5|1181007882| 2.9585264|\n",
      "|   233|   1580|   3.0|1529334057|  2.989352|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                diff|\n",
      "+--------------------+\n",
      "|  2.8579143285751343|\n",
      "|  0.9201302528381348|\n",
      "|  1.2862935066223145|\n",
      "|  0.9543664455413818|\n",
      "|  2.9175024032592773|\n",
      "|  1.7934308052062988|\n",
      "|-0.21140527725219727|\n",
      "|  2.1195260286331177|\n",
      "| -1.3107528686523438|\n",
      "| 0.38820719718933105|\n",
      "| -0.5348916053771973|\n",
      "|  -2.182537078857422|\n",
      "|  0.7891116142272949|\n",
      "|  1.3964476585388184|\n",
      "| -0.5715861320495605|\n",
      "|  -2.191871166229248|\n",
      "| -0.6360106468200684|\n",
      "|  0.1282951831817627|\n",
      "|  -1.458526372909546|\n",
      "|0.010648012161254883|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions.select('rating').subtract(predictions.select('prediction')).collect()\n",
    "predictions.withColumn('diff',predictions['rating']-predictions['prediction']).select('diff').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# predictions.withColumn('diff',predictions['rating']-predictions['prediction']).select(avg('diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName = 'rmse', labelCol = 'rating', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|userId|movieId|\n",
      "+------+-------+\n",
      "|    12|   1265|\n",
      "|    12|   2100|\n",
      "|    12|   2485|\n",
      "|    12|   3967|\n",
      "|    12|   6942|\n",
      "|    12|  40629|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_user = test.filter(test['userId'] == 12).select('userId', 'movieId')\n",
    "test_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|    12|   1265| 4.9313936|\n",
      "|    12|   2485| 3.5849001|\n",
      "|    12|   3967| 4.9177637|\n",
      "|    12|  40629| 7.2070017|\n",
      "|    12|   2100| 3.6872206|\n",
      "|    12|   6942|  4.566821|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendation_test_user = model.transform(test_user)\n",
    "recommendation_test_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|    12|  40629| 7.2070017|\n",
      "|    12|   1265| 4.9313936|\n",
      "|    12|   3967| 4.9177637|\n",
      "|    12|   6942|  4.566821|\n",
      "|    12|   2100| 3.6872206|\n",
      "|    12|   2485| 3.5849001|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendation_test_user.orderBy('prediction', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
